{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PIPELINE**\n",
    "\n",
    "En esta última práctica, veremos cómo realizar la puesta en producción de un modelo de Machine Learning.\n",
    "\n",
    "El pipeline es una forma de codificar y automatizar el workflow necesario para producir un modelo de Machine Learning. Los pipelines en ML constan de varios pasos secuenciales que hacen de todo, desde la extracción y el preprocesamiento de datos hasta el entrenamiento y la implementación de modelos.\n",
    "\n",
    "En definitiva, nos ayuda a automatizar un flujo de trabajo de Machine Learning.\n",
    "\n",
    "<img src = \"./_src/assets/pipeline_2.jpg\" height = 350>\n",
    "\n",
    "Uno de sus grandes beneficios y aplicabilidad reside en la escalabilidad. A su vez, destaca entre sus ventajas que simplifica la aplicación de cross-validation y optimización de hiperparámetros.\n",
    "\n",
    "Con este enfoque, podemos *empaquetar* todas las acciones que van desde el preprocesamiento de los datos hasta las predicciones del modelo en un único objeto.\n",
    "\n",
    "La librería `Scikit-learn` incluye una clase, denominada precisamente **pipeline**, que está diseñada para poder manipular y aplicar una serie de transformaciones de datos seguida de la aplicación de un modelo de ML. En otras palabras, con esta clase podemos instanciar un objeto que incluya todo el flujo del preprocesamiento de datos -desde la limpieza, normalización, escalado, transformación, reducción de dimensionalidad hasta ingeniería de features- y el modelo predictivo con el cual haremos las inferencias.\n",
    "\n",
    "También Scikit-learn nos brinda la posibilidad de hacer nuestro pipeline con la clase **make_pipeline**. Veamos brevemente en qué se diferencia una de la otra.\n",
    "\n",
    "|Pipeline|Make_pipeline|\n",
    "|-|-|\n",
    "|Hay que especificar todos los pasos secuenciales en una lista|No hay que indicar un nombre para cada paso|\n",
    "|Cada paso es una tupla con el nombre del paso y su nombre oficial en Scikit-learn|Se pueden encadenar todos los pasos usando sus nombres oficiales en Scikit-learn|\n",
    "|Ejemplo: `a`|Ejemplo: `b`|\n",
    "\n",
    "```python\n",
    "\n",
    "'a' Pipeline([('imputer', SimpleImputer(strategy='median')),('scaler', StandardScaler())])\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "\n",
    "'b' make_pipeline(SimpleImputer(strategy='median'), StandardScaler())\n",
    "\n",
    "```\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Práctica`\n",
    "\n",
    "Para la realización de esta práctica construiremos tres pipelines, cada uno con un modelo diferente. \n",
    "\n",
    "Primero haremos algunos pasos del preprocesamiento de datos, luego adaptaremos nuestros modelos. Para finalizar, compararemos la performance de los modelos y buscaremos identificar el mejor de ellos. Una vez obtenido, lo guardamos en un archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos con los que vamos a estar trabajando\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos los datos en train y test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, crearemos los tres pipelines. Los modelos que usaremos serán: regresión logística, support vector machine y árbol de decisión. Respecto al preprocesamiento, aplicaremos un escalado de datos con `Standard Scaler` y haremos reducción de dimensionalidad con `PCA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el primer pipeline\n",
    "\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()), # estandarización de datos \n",
    "\t\t\t('pca', PCA(n_components=2)), # reducción de dimensionalidad con PCA previa a la regresión logística\n",
    "\t\t\t('clf', LogisticRegression(random_state=42))]) # pipeline de regresión logística con PCA y estandarización de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el segundo pipeline\n",
    "\n",
    "pipe_svm = Pipeline([('scl', StandardScaler()), # estandarización de datos \n",
    "\t\t\t('pca', PCA(n_components=2)), # reducción de dimensionalidad con PCA previa a SVM\n",
    "\t\t\t('clf', svm.SVC(random_state=42))]) # pipeline con SVM como clasificador final y PCA como reductor de dimensionalidad previo al clasificador final (SVM) \n",
    "\t\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el tercer pipeline\n",
    "\n",
    "pipe_dt = Pipeline([('scl', StandardScaler()), # estandarización de datos\n",
    "\t\t\t('pca', PCA(n_components=2)), # reductor de dimensionalidad previo al clasificador final (árbol de decisión) \n",
    "\t\t\t('clf', tree.DecisionTreeClassifier(random_state=42))]) # Creamos el pipeline con los parámetros que queramos para cada paso del mismo (en este caso, el clasificador es un árbol de decisión) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los guardamos en una lista\n",
    "\n",
    "pipelines = [pipe_lr, pipe_svm, pipe_dt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos un diccionario para fines organizativos\n",
    "\n",
    "pipe_dict = {0: 'Regresión Logística', 1: 'SVM', 2: 'Árbol de decisión'} # Diccionario para fines organizativos de los resultados de los modelos de clasificación binaria (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos\n",
    "\n",
    "for pipe in pipelines:\n",
    "\tpipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Regresión Logística pipeline accuracy en test: 0.900\n",
      "1\n",
      "SVM pipeline accuracy en test: 0.900\n",
      "2\n",
      "Árbol de decisión pipeline accuracy en test: 0.867\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos\n",
    "\n",
    "for idx, val in enumerate(pipelines):\n",
    "\tprint(idx)\n",
    "\tprint('%s pipeline accuracy en test: %.3f' % (pipe_dict[idx], val.score(X_test, y_test))) # Imprimimos la precisión de cada modelo de clasificación binaria (0,1) en test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo con el mejor accuracy: Regresión Logística\n"
     ]
    }
   ],
   "source": [
    "# Identificamos el mejor modelo para el set de testeo\n",
    "\n",
    "best_acc = 0.0\n",
    "best_clf = 0\n",
    "best_pipe = ''\n",
    "for idx, val in enumerate(pipelines):\n",
    "\tif val.score(X_test, y_test) > best_acc:\n",
    "\t\tbest_acc = val.score(X_test, y_test)\n",
    "\t\tbest_pipe = val\n",
    "\t\tbest_clf = idx\n",
    "print('Modelo con el mejor accuracy: %s' % pipe_dict[best_clf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline de Regresión Logística guardado a archivo\n"
     ]
    }
   ],
   "source": [
    "# Guardamos el pipeline en un archivo\n",
    "# import pickle\n",
    "import joblib #pickle es alternativa\n",
    "\n",
    "joblib.dump(best_pipe, 'Mejor_pipeline.pkl', compress=1) # Guardamos el pipeline en un archivo para su uso posterior.\n",
    "print('Pipeline de %s guardado a archivo' % pipe_dict[best_clf]) # Imprimimos el nombre del pipeline guardado en el archivo para fines organizativos \n",
    "\n",
    "loaded_model = joblib.load('Mejor_pipeline.pkl') # Cargamos el pipeline guardado en un archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mejor_clasificador = joblib.load('Mejor_pipeline.pkl') # Cargamos el pipeline que guardamos en el archivo Mejor_pipeline.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar con pickle\n",
    "# pickle.dump(best_pipe, open('Mejor_pipeline.pkl', 'wb'))\n",
    "\n",
    "# Cargamos el pipeline\n",
    "\n",
    "# import pickle\n",
    "# loaded_model = pickle.load(open('Mejor_pipeline.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargamos nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = joblib.load('Mejor_pipeline.pkl') # Cargamos el pipeline guardado en un archivo \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=100) # Separamos los datos en train y test donde vamos a predecir con el modelo guardado en el archivo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy del modelo cargado: 0.908\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy del modelo cargado: %.3f' % best_model.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy del modelo guardado: 0.933\n"
     ]
    }
   ],
   "source": [
    "best_model.score(X_test, y_test)\n",
    "print('Accuracy del modelo guardado: %.3f' % best_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "9b0261dadc93494a3555537365c322d83416c4a1ed03d5df7c77bc94b07686c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
